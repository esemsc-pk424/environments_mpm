{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assessment 1**\n",
        "\n",
        "For our first assessment, our goal is to solve an imputation problem: we will create a neural network architecture that learns how to recover missing portions of an image.\n",
        "\n",
        "This is an important problem in magnetic resonance imaging (MRI), where patient scans are often limited to a few areas to avoid lengthy scanning times.\n",
        "\n",
        "In particular, we are going to focus on images of human heads. We have managed to gain access to one hundred images of patient's heads but, unfortunately, these images have a significant portion of missing information. Your goal during the assessment is to design a neural network that can recover these missing portions.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "We do not have access to the labels for the images we want to recover, so we will have to be a bit creative to obtain a workable dataset on which to train our neural network.\n",
        "\n",
        "Fortunately for us, we have access to a generative model that has been trained to produce realistic-looking MRI images of patient's heads. Using this model, you will create an appropriate dataset to train your architecture. We have provided you with the basic setup code to start using this generative model in **Question 1** below.\n",
        "\n",
        "The corrupted images that we want to recover are contained in the numpy file `test_set.npy` of this repository. The file contains 100 patient images with a size of 64x64 pixels.\n",
        "\n",
        "The architecture that you design in this assessment should use the artificially-generated dataset in order to recover the missing information in the images contained in `test_set.npy`.\n",
        "\n",
        "<br>\n",
        "\n",
        "All answers to the assessment should be contained within the structure below, but you are free to add new code and text cells as required to your answers. Read the text for each question and follow the instructions carefully. Answers that do not follow this structure will not be marked. **Do NOT change the name of this file.**\n",
        "\n",
        "Please, **make sure to execute all your cells and save the result of the execution**. We will only mark cells that have been executed and will not execute any cells ourselves.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "-QqVRXQpJ8pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1**  (25%)\n",
        "\n",
        "Using the provided image-generation network, create a dataset of brain images that will later be used to train your chosen architecture.\n",
        "\n",
        "Given that you will likely want to use this dataset multiple times during training, we recommend that you save the generated images to an appropriate folder in your GDrive.\n",
        "\n",
        "Once you have generated your dataset, load and display ten of your generated images here.\n",
        "\n",
        "We have also provided you with some corrupted images in the file `test_set.npy` of this repository. You should also load and display ten of these corrupted images here.\n",
        "\n",
        "Below, we have provided template code, including some required downloads and installations, so that you can easily use the trained generative model. Sample generation in this model is done using the function `generate`, and is controlled by some input arguments. It is your job to figure out a sensible set of parameters that will produce images that are useful for the requirements of your task.\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "N7wmUvz7Ssz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDzTDiZyJ6Fp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "RSz63tFpUoNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull and install necessary files\n",
        "!gdown 1-6JPxY7rUySRaCXTKHD400e8UWs2DZiY # run.sh\n",
        "!gdown --folder 1-PJmUzB9HN3mCKsl7qKGlUynltY5ttp7 # ese-invldm\n",
        "!gdown --folder 1-V9EOGuiYOiszIg3pA15D9ThwXO1gr4E # files\n",
        "!chmod +x run.sh\n",
        "!bash ./run.sh\n",
        "sys.path.append('/content/ese-invldm')"
      ],
      "metadata": {
        "id": "A0Whr9g1Vffe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ese_invldm import generate\n",
        "\n",
        "\"\"\"\n",
        "Generates samples using a diffusion-based generative model.\n",
        "\n",
        "This function leverages a pre-configured diffusion model to produce synthetic samples.\n",
        "The sampling process supports adjustable parameters for total samples, inference steps, and batch size.\n",
        "A scheduler and temperature can also be configured to control the sampling behaviour.\n",
        "\n",
        "Parameters:\n",
        "    num_samples (int):\n",
        "        Total number of samples to generate.\n",
        "    num_inference_steps (int):\n",
        "        Number of diffusion inference steps.\n",
        "        The minimum number of steps is 1, but we recommend exploring the range from 10 to 50.\n",
        "        Please note that more steps will increase quality but also the computational cost. Be careful not\n",
        "        to burn through your credits by using a very large number of steps!\n",
        "    batch_size (int):\n",
        "        Number of samples to process in each batch during sampling.\n",
        "    scheduler (str, optional):\n",
        "        Sampling scheduler to use (e.g., \"ddim\", \"ddpm\"). You can quickly test which one provides the most appropriate results\n",
        "        for this task.\n",
        "    temperature (float, optional):\n",
        "        Sampling temperature to control randomness, given as a number between 0 and 1. Higher values produce more diverse outputs.\n",
        "    seed (int, optional):\n",
        "        Random seed for reproducibility. Defaults to 42.\n",
        "\n",
        "Returns:\n",
        "    list:\n",
        "        A list containing the batches of generated samples, where each sample\n",
        "        corresponds to a single data instance produced by the diffusion model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "samples = generate(##)"
      ],
      "metadata": {
        "id": "6KbXjEepVhTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Question 2**  (25%)\n",
        "\n",
        "Using the data generated in **Question 1**, create a PyTorch `TensorDataset` and a `DataLoader` for the training set.\n",
        "\n",
        "Using the provided corrupted images inside `test_set.npy`, create another `TensorDataset` and a `DataLoader` for the test set.\n",
        "\n",
        "The training dataset should provide batches of brain images generated in **Question 1** and should corrupt these images appropriately so that they resemble images in the test set. The dataset should also pair each image with its corresponding un-corrupted image as a label.\n",
        "\n",
        "The test dataset should provide the corrupted images provided, for which no labels are available.\n",
        "\n",
        "Display here ten images of your training dataset and ten images of your test dataset, and their corresponding labels when available.\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "mBVr6D2oSInX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuETd-X2SKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Question 3** (50%)\n",
        "\n",
        "Using the dataset created in **Question 2**, design and train an architecture to recover the missing image lines of the provided test dataset.\n",
        "\n",
        "Once you have trained your architecture, display here ten images of the test set with the recovered lines filled in.\n",
        "\n",
        "Additionally, save the test data with the missing values filled in into a numpy file called `test_set_nogaps.npy`. These images should be **in the same order** as those in the `test_set.npy` file and should have the same pixel size of 64x64. **Any images not contained in the `test_set_nogaps.npy` file or incorrectly ordered will not be marked.**\n",
        "\n",
        "You have freedom to choose an architecture that you consider appropriate to solve this problem. However, you will need to train your chosen architecture as part of the assessment: **pre-trained networks are not allowed**.\n",
        "\n",
        "You will be assessed by the quality of your predictions of the missing data values and additional marks will be given for originality in your network design choices. You should include, as part of your answer, a paragraph explaining the architecture you have chosen and any additional design choices and hyperparameters that have been important to build your solution.\n",
        "\n",
        "This is an open-book assessment and you are encouraged to use resources online, including  tools like chatGPT. However, make sure to always mention the sources for your code and ideas, including websites, papers, and tools like chatGPT.\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "1-AoRoLUSKoH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFNwYx_hSMG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "5cdZQIkWu0_r"
      }
    }
  ]
}